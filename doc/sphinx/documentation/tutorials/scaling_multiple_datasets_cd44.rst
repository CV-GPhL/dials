Scaling in detail - cd44
=============================================

.. highlight:: none

This tutorial discusses scaling the cd44 dataset from the ccp4 tutorial,
which can currently be found at http://www.ccp4.ac.uk/tutorials/targets/standard/

After integration, the next step is scaling of the reflection intensities,
which attempts to correct the measured intensities to the 'true' scattering
intensities due to the contents of the crystal unit cell. This correction is
necessary as effects such as changes in sample illumination, absorption and
radiation damage during the experiment result in symmetry-equivalent reflections
having unequal measured intensities.

For scaling multiple datasets in DIALS, there are two workflows. All datasets can
be scaled simultaneously, by passing all :samp:`integrated.pickle` and
:samp:`integrated_experiments.json` files to dials.scale. Alternatively, datasets
can be incrementally scaled and added to a combined set of reflections.
In this tutorial, we shall scale the datasets incrementally, which allows more
control over the scaling models and options for each dataset.

Scaling the first dataset
^^^^^^^^^^^^^^^^^^^^^^^^^

First, the datafiles are passed to dials.scale::

  dials.scale 14_integrated_experiments.json 14_integrated.pickle

In order to perform scaling, a scaling model must be created,
from which we can calculate an inverse scale factor :math:`g_i` for each reflection.
(i.e. the corrected intensities are given by :math:`I^{cor}_i = I^{obs}_i / g_i`)
By default, three components are used to create a physical model for scaling
(:samp:`model=physical`), in a similar manner to that used in the
program aimless_. This model consists of a smoothly varying scale factor as a
function of rotation angle (:samp:`scale_term`), a smoothly varying B-factor to
account for radiation damage as a function of rotation angle (:samp:`decay_term`)
and an absorption surface correction, dependent on the direction of the incoming
and scattered beam vector relative to the crystal (:samp:`absorption_term`).
Each component typically consists of 10 to 40 parameters, and during scaling
these parameters are refined so that the symmetry-equivalent groups of corrected
intensities have the best agreement across the dataset.

Once the scaling has been performed, the updated dataset is saved to
:samp:`scaled.pickle`, while details of the scaling model are saved in an
updated experiments file :samp:`scaled_experiments.json`.

Inspecting the results
^^^^^^^^^^^^^^^^^^^^^^

A summary of the merging statistics after scaling is given is the output::

             ----------Overall merging statistics (non-anomalous)----------        

  Resolution: 25.81 - 1.69

  Observations: 94892

  Unique reflections: 27557

  Redundancy: 3.4

  Completeness: 72.15%

  Mean intensity: 69.4

  Mean I/sigma(I): 6.5

  R-merge: 0.115

  R-meas:  0.133

  R-pim:   0.067

To see what the scaling is telling us about the dataset, plots of the scaling
model should be viewed. These can be generated by passing the output files to
the utility program dials.plot_scaling_models::

  dials.plot_scaling_models scaled_experiments.json scaled.pickle

  open scale_model.png absorption_surface.png

.. image:: /figures/scaling_cd44.png

As the rotation angle increases, the inverse scale factor drops significantly
(i.e the reflection intensities are getting weaker, so they need to be scaled
up by dividing by a smaller inverse scale factor). This indicates some
deterioration of the dataset towards the end of the rotation (a downward trend in the
B-factor can also be seen), likely due to radiation damage.
The absorption correction appears to indicate some
anisotropy in the vertical direction, therefore it seems prudent to keep this
correction (it could be turned off with the option :samp:`absorption_term=False`).
To omit the most damaged images, scaling can be rerun
with the option :samp:`exclude_image_range`. As an example, we shall cut the
last 20 images::

  dials.scale 14_integrated_experiments.json 14_integrated.pickle exclude_image_range=120,140

::

             ----------Overall merging statistics (non-anomalous)---------- 
    
  Resolution: 25.12 - 1.69

  Observations: 81463

  Unique reflections: 27065

  Redundancy: 3.0

  Completeness: 70.86%

  Mean intensity: 69.8

  Mean I/sigma(I): 6.9

  R-merge: 0.104

  R-meas:  0.124

  R-pim:   0.066


The merging statistics have changed slightly (Rpim has slightly decreased from
0.067 to 0.066), possibly as a result of the reduced number of reflections.
Unfortunately, the benefits of a given cutoff choice may only become apparent
by comparing the results of full structure solution, but this seems like a
reasonable choice for this dataset at this stage.

Adding a second dataset
^^^^^^^^^^^^^^^^^^^^^^^

Once we are happy with the first dataset, we can add in the second dataset. As this is
a much thinner rotation wedge of 15 degrees, we should change the interval of the
smoothly varying parameterisation, to have a finer sampling, such as at 5 degree intervals
:samp:`scale_interval=5, decay_interval=5`. It also does not usually
make sense to use an absorption surface for such a small wedge dataset, so let's
set :samp:`absorption_term=False`.
For scaling the datasets together,
we want to take advantage of the best intensity estimates determined from the previous
scaling run, therefore we start from where we left off by passing in the
:samp:`scaled.pickle` and :samp:`scaled_experiments.json` files. 
The full command is thus (typed all at once before hitting enter)::

  dials.scale 30_integrated_experiments.json 30_integrated.pickle
  scaled_experiments.json scaled.pickle scale_interval=5, decay_interval=5
  absorption_term=False

During scaling, a first round of scaling is performed, where the model of the new
dataset is refined against the intensity estimates from the previous dataset(s),
before all scaling models are refined simultaneously. This allows quicker convergence, particularly
for large datasets. All reflections and scaling model data are saved to one
:samp:`scaled.pickle` and :samp:`scaled_experiments.json` file, allowing the
scaling process to be repeated for additional datasets if applicable.
At this stage, we could have also included a min/max resolution cutoff, with the
:samp:`d_min=`, :samp:`d_max=` options.

Including the second dataset has improved the overall Rpim to 0.062, and plotting
the scaling models shows minimal variation in scale factors across the sweep.
For this dataset, or for a thinner rotation wedge, :samp:`model=KB` may be an 
appropriate choice - for this model, a global scale factor and global B factor is applied
to all reflections in that dataset. This may be preferable to avoid overfitting a physical model
by using increasingly small parameter intervals, and will result in faster scaling
for large or many datasets.

Exporting for further processing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Once we are happy with the results from scaling, the data can be exported as
an unmerged mtz file, for further symmetry analysis with pointless_ or to start
structural solution.
To obtain an unmerged mtz file, :samp:`dials.export` should be run, passing in
the output from scaling, with the option :samp:`intensity=scale`::

  dials.export scaled.pickle scaled_experiments.json intensity=scale


Scaling several similar datasets
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If one has several datasets, the easiest option would be to scale all datasets
together from the start::

  dials.scale *_integrated_experiments.json *_integrated.pickle

This will pass in all :samp:`integrated_experiments.json` and
:samp:`integrated.pickle` files from the current directory and perform joint
scaling. While a different scaling model is used for each dataset, the model 
options used will be the same (e.g :samp:`scale_interval`), which may not be
the most appropriate for all datasets if there is a mix of thin/wide datasets.

Practicalities for large datasets
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Depending on the computational resources available, scaling of large datasets
( > 1 million reflections) can become slow and memory intensive.
There are several options available for managing this.
The first option is separating the data in memory to allow blockwise calculations
and parallel processing, using the option :samp:`nproc=` (a value of 4 or 8 is probably a
reasonable choice).
One of the most intensive part of the algorithm is
full matrix minimisation, which is by default performed after a quicker LBFGS
minimisation round. One can set :samp:`full_matrix=False` to turn this off, however
no errors for the inverse scale factors will be determined. A compromise is
to set :samp:`full_matrix_max_iterations=1` to do at least one iteration.
A third option is to reduce the number of reflections used by the scaling
algorithm during minimisation. By default, a subset of reflections is chosen based on their
normalised intensities, with the default set chosen between E2 values of 0.8
and 5.0, which typically selects between 1/3 and 1/2 of the dataset. These limits
can be set with :samp:`E2_min=` and :samp:`E2_max=`, or similary an
:samp:`Isigma_range` or :samp:`d_range` can be set to reduce the number of reflections
used to determine the scaling model. However, one should be
careful that the subset is representative of the whole dataset, and selecting
too few reflections will lead to overfitting of the subset and worse overall
merging statistics.

.. _aimless: http://www.ccp4.ac.uk/html/aimless.html
.. _pointless: http://www.ccp4.ac.uk/html/pointless.html
